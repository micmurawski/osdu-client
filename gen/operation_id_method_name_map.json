{
    "get:/_ah/liveness_check": [
        "get_liveness_check",
        "livenessCheck",
        ""
    ],
    "get:/_ah/readiness_check": [
        "get_readiness_check",
        "readinessCheck",
        ""
    ],
    "get:/groups": [
        "list_groups",
        "listGroups",
        ""
    ],
    "post:/groups": [
        "create_group",
        "createGroup",
        ""
    ],
    "delete:/groups/{group_email}": [
        "delete_group",
        "deleteGroup",
        ""
    ],
    "patch:/groups/{group_email}": [
        "patch_groups",
        "updateGroup",
        ""
    ],
    "get:/groups/{group_email}/members": [
        "list_group_members",
        "listGroupMembers",
        ""
    ],
    "post:/groups/{group_email}/members": [
        "add_member",
        "addMember",
        ""
    ],
    "delete:/groups/{group_email}/members/{member_email}": [
        "delete_member_from_group",
        "deleteMember",
        ""
    ],
    "delete:/members/{member_email}": [
        "delete_member",
        "deleteMember",
        ""
    ],
    "get:/members/{member_email}/groups": [
        "list_member_groups",
        "listGroupsOnBehalfOf",
        ""
    ],
    "post:/tenant-provisioning": [
        "initiate_tenant",
        "initiateTenant",
        ""
    ],
    "get:/api/entitlements/v2/groups/all": [
        "list_partition_groups",
        "listAllPartitionGroups",
        ""
    ],
    "get:/info": [
        "get_info",
        "Version info",
        "For deployment available public `/info` endpoint, \\ \\ which provides build and git related information."
    ],
    "get:/groups/{group_email}/membersCount": [
        "get_count_group_members",
        "countGroupMembers",
        ""
    ],
    "get:/svcstatus": [
        "get_svcstatus",
        "Seismic store service status (fast check).",
        "<ul><li>Return the seismic store service status.</li><li>Required roles: none</li></ul>"
    ],
    "get:/svcstatus/access": [
        "get_svcstatus_access",
        "Seismic store service access check.",
        "<ul><li>Validates if the token audience is allowed</li><li>Required roles: none</li></ul>"
    ],
    "post:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}": [
        "register_new_dataset",
        "Register a new dataset.",
        "<ul><li>Register a new dataset in the seismic store.</li><li>Required roles: subproject.admin</li></ul>"
    ],
    "get:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}": [
        "get_dataset",
        "Retrieve a dataset.",
        "<ul> <li>Return the dataset metadata from seismic store.</li> <li>Required roles: <ul> <li>subproject.admin, subproject.viewer: if the applied subproject policy is 'uniform'</li> <li>dataset.admin, dataset.viewer: if the applied subproject policy is 'dataset'</li> </ul> </li></ul>"
    ],
    "delete:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}": [
        "delete_dataset",
        "Delete a dataset.",
        "<ul> <li>Delete a dataset in the seismic store.</li> <li>Required roles: <ul> <li>subproject.admin: if the applied subproject policy is 'uniform'</li> <li>dataset.admin: if the applied subproject policy is 'dataset'</li> </ul> </li></ul>"
    ],
    "patch:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}": [
        "patch_dataset_metadata",
        "Patch the dataset metadata.",
        "<ul>\n<li>Update the dataset meta information in the seismic store or close (unlock) the dataset. If the endpoint is used without the close parameter, at least one body field is required or the endpoint will return an error.</li>\n<li>Required roles:\n  <ul>\n    <li>subproject.admin: if the applied subproject policy is 'uniform'</li>\n    <li>dataset.admin: if the applied subproject policy is 'dataset'</li>\n  </ul>\n</li>\n<li>Patchable fields: <ul>\n<li><b>dataset_new_name:</b> new name to use for the dataset (rename).</li>\n<li><b>filemetadata:</b> This is a seismic store specific field and describes how the physical data is stored in the cloud storage system (GCS/AzureContainer etc.). This metadata is mainly used by client libraries to correctly reconstruct the dataset. For example you can store a dataset as truncated in multiple objects of 64MB each, name them from 0 to N and save the filemetadata = \u201c{nObject: N, totalSize: 1024, objsize: 64, sizeUnit: MB}\u201d.</li>\n<li><b>gtags:</b> Upsert tags to an existing dataset metadata. If the dataset metadata already has gtags, then new gtags are appended to this list.</li>\n<li><b>ltag:</b> Update the existing legalTag value.</b></li>\n<li><b>readonly:</b> Update the dataset mode to readonly (true) or to read/write (false).</li>\n<li><b>status:</b> Update the dataset status.</li>\n</ul></li>\n<li><b>NOTE:</b> last_modified_date is updated automatically with each metadata change through Patch endpoint calling.</li>\n</ul>\n"
    ],
    "put:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}/lock": [
        "acquire_lock_for_dataset_id",
        "Acquire a lock for a dataset id.",
        "<ul>\n  <li>Open a dataset for read or write and lock its state.</li>\n  <li>Required roles open lock for write:\n    <ul>\n      <li>subproject.admin: if the applied subproject policy is 'uniform'</li>\n      <li>dataset.admin: if the applied subproject policy is 'dataset'</li>\n    </ul>\n  </li>\n  <li>Required roles open lock for read:\n    <ul>\n      <li>subproject.admin, subproject.viewer: if the applied subproject policy is 'uniform'</li>\n      <li>dataset.admin, dataset.viewer: if the applied subproject policy is 'dataset'</li>\n    </ul>\n  </li>\n</ul>\noperationId: dataset-lock\n"
    ],
    "put:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}/unlock": [
        "remove_lock_associated_with_dataset_id",
        "Remove the lock associated with a dataset id.",
        "<ul>\n  <li>Removes the lock for a dataset id.</li>\n  <li>Required roles:\n    <ul>\n      <li>subproject.admin: if the applied subproject policy is 'uniform'</li>\n      <li>dataset.admin: if the applied subproject policy is 'dataset'</li>\n    </ul>\n  </li>\n</ul>\n"
    ],
    "get:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}/permission": [
        "get_access_permissions_user_on_dataset_id",
        "Retrieve the access permissions of a user on a dataset id.",
        "<ul>\n  <li>Retrieve the access permission of a user on a dataset.</li>\n  <li>Required roles:\n    <ul>\n      <li>subproject.admin, subproject.viewer: if the applied subproject policy is 'uniform'</li>\n      <li>dataset.admin, dataset.viewer: if the applied subproject policy is 'dataset'</li>\n    </ul>\n  </li>\n</ul>\n"
    ],
    "get:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}/ctagcheck": [
        "get_dataset_tenant_subproject_dataset_ctagcheck",
        "Validate if a dataset ctag matches the pre-existing ctag in metadata catalog.",
        "<ul><li>Check if the provided dataset cTag match the one stored in the metadata catalog.</li><li>Required roles: subproject.admin, subproject.viewer</li></ul>"
    ],
    "put:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}/gtags": [
        "upsert_tags_to_dataset",
        "Upsert tags to a dataset.",
        "<ul>\n  <li>Upsert tags to an existing dataset metadata. If the dataset metadata already has gtags, then  new gtags are appended to this list.</li>\n  <li>Required roles:\n    <ul>\n      <li>subproject.admin: if the applied subproject policy is 'uniform'</li>\n      <li>dataset.admin: if the applied subproject policy is 'dataset'</li>\n    </ul>\n  </li>\n</ul>\n"
    ],
    "post:/dataset/tenant/{tenantid}/subproject/{subprojectid}/dataset/{datasetid}/size": [
        "compute_and_get_size_dataset",
        "Compute and retrieve the size of a dataset and the date of when the size was computed",
        "<ul>\n<li>Compute and retrieve the size of a dataset and the date of when the size was computed.</li>\n<li>Required roles: subproject.admin</li></ul>\n"
    ],
    "get:/dataset/tenant/{tenantid}/subproject/{subprojectid}/size": [
        "get_datasets_size",
        "Retrieve the size of datasets",
        "<ul>\n<li>Required roles: subproject.viewer</li></ul>\n"
    ],
    "get:/dataset/tenant/{tenantid}/subproject/{subprojectid}/readdsdirfulllist": [
        "get_content_list",
        "Content list.",
        "<ul>\n<li>List datasets and sub-directories for a directory path.</li>\n<li>Required roles: subproject.admin, subproject.viewer</li></ul>\n"
    ],
    "post:/dataset/tenant/{tenantid}/subproject/{subprojectid}": [
        "list_datasets_in_subproject",
        "Get the list of datasets in a subproject.",
        "<ul>\n  <li>Return the list of datasets in a sub-project. The list can be filtered by gtags. Support pagination.</li>\n  <li>Required roles: subproject.admin, subproject.viewer</li>\n</ul>\n"
    ],
    "post:/dataset/tenant/{tenantid}/subproject/{subprojectid}/exist": [
        "create_dataset_tenant_subproject_exist",
        "Check to see if a list of datasets exists in the subproject.",
        "<ul>\n<li>Check if the dataset exists.</li>\n<li>Required roles: subproject.admin, subproject.viewer</li></ul>\n"
    ],
    "post:/dataset/tenant/{tenantid}/subproject/{subprojectid}/sizes": [
        "get_datasets_sizes",
        "Retrieve the size of datasets.",
        "<ul>\n<li>Return a list with the sizes of the requested datasets.</li>\n<li>The correctness is not guarantee since this API returns sizes stored by the user in the dataset manifest.</li>\n<li>This API is deprecated, please using /size endpoint to compute and retrieve the size information</li>\n<li>Required roles: subproject.admin, subproject.viewer</li></ul>\n"
    ],
    "get:/utility/ls": [
        "list_datasets",
        "Retrieve the list of datasets and sub-directories inside a seismic store path.",
        "<ul>\n<li>Return the list of datasets and sub-directories of a seismic store path.</li>\n<li>Required roles: subproject.admin, subproject.viewer</li></ul>\n"
    ],
    "post:/utility/ls": [
        "list_post_datasets",
        "Retrieve the list of datasets and sub-directories inside a seismic store path.",
        "<ul>\n<li>Return the list of datasets and sub-directories of a seismic store path.</li>\n<li>Required roles: subproject.admin, subproject.viewer</li></ul>\n"
    ],
    "get:/utility/storage-tiers": [
        "get_utility_storage_tiers",
        "Retrieve the list of supported storage tiers",
        "<ul><li>Return the list of storage tiers</ul>"
    ],
    "post:/utility/cp": [
        "copy_dataset",
        "Copy dataset.",
        "<ul>\n<li>Copy a seismic store dataset. The source and destination dataset must be in the same sub-project.</li>\n<li>Required roles:\n  <ul>\n    <li>subproject.admin, subproject.viewer: if the applied subproject policy is 'uniform'</li>\n    <li>dataset.admin, dataset.viewer: if the applied subproject policy is 'dataset'</li>\n  </ul>\n</li>\n</ul>\n"
    ],
    "get:/utility/gcs-access-token": [
        "get_utility_gcs_access_token",
        "Generate a GCS access token.",
        "<ul>\n  <li>Generate a GCS access token for a specified seismic store resource. The source and destination dataset must be in the same sub-project.</li>\n  <li>Required roles:\n    <ul>\n      <li>subproject.admin, subproject.viewer: if the applied subproject policy is 'uniform'</li>\n      <li>dataset.admin, dataset.viewer: if the applied subproject policy is 'dataset'</li>\n    </ul>\n  </li>\n</ul>\n"
    ],
    "get:/utility/upload-connection-string": [
        "get_utility_upload_connection_string",
        "Generate the upload connection credentials string",
        "<ul> <li>Generate the upload connection credential string for a subproject collection or a dataset, depending of the applied access policy (uniform/dataset). <li>These credentials can be used via CSP SDK, on client side, to perform bulk upload.</li> <li> The endpoint response is CSP (Cloud Solution Provider) dependent: <ul> <br/><li><b>Azure</b>: shared access signature (SaS) Url token <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'https://{accountName}.blob.core.windows.net/{containerName}?{SASQueryParameters}`' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 3599 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'SasUrl' <br/>} <br/><br/></li> <li><b>Google</b>: standard access token credential signed and down-scoped by google <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'google_signed_access_token' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 3600 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'Bearer' <br/>} <br/><br/></li> <li><b>AWS</b>: double column separated string containing access key id, the access key secret and the session token <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'accessKeyId:secretAccessKey:sessionToken' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 3599 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'Bearer' <br/>} <br/><br/></li> <li><b>IBM</b>: double column separated string containing access key id, the access key secret and the session token <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'accessKeyId:secretAccessKey:sessionToken' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 7200 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'Bearer' <br/>} <br/><br/></li> </ul> </li> <li>Required roles: <ul> <li>subproject.admin: if the applied subproject policy is 'uniform'</li> <li>dataset.admin: if the applied subproject policy is 'dataset'</li> </ul> </li> </ul>"
    ],
    "get:/utility/download-connection-string": [
        "get_utility_download_connection_string",
        "Generate the download connection credentials string",
        "<ul> <li>Generate the download connection credential string for a subproject collection or a dataset, depending of the applied access policy (uniform/dataset). <li>These credentials can be used via CSP SDK, on client side, to perform bulk download.</li> <li> The endpoint response is CSP (Cloud Solution Provider) dependent: <ul> <br/><li><b>Azure</b>: shared access signature (SaS) Url token <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'https://{accountName}.blob.core.windows.net/{containerName}?{SASQueryParameters}`' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 3599 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'SasUrl' <br/>} <br/><br/></li> <li><b>Google</b>: standard access token credential signed and down-scoped by google <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'google_signed_access_token' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 3600 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'Bearer' <br/>} <br/><br/></li> <li><b>AWS</b>: double column separated string containing access key id, the access key secret and the session token <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'accessKeyId:secretAccessKey:sessionToken' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 3599 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'Bearer' <br/>} <br/><br/></li> <li><b>IBM</b>: double column separated string containing access key id, the access key secret and the session token <br/> <br/>{ <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;access_token: 'accessKeyId:secretAccessKey:sessionToken' <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires_in: 7200 <br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_type: 'Bearer' <br/>} <br/><br/></li> </ul> </li> <li>Required roles: <ul> <li>subproject.admin, subproject.viewer: if the applied subproject policy is 'uniform'</li> <li>dataset.admin, dataset.viewer: if the applied subproject policy is 'dataset'</li> </ul> </li> </ul>"
    ],
    "post:/imptoken": [
        "create_imptoken",
        "Generate impersonation credentials token.",
        "<ul>\n<li>Generate an impersonation credential token of a user for a set of subproject resources.</li>\n<li>Required roles: app.trusted</li></ul>\n"
    ],
    "put:/imptoken": [
        "update_imptoken",
        "Refresh an impersonation credential token.",
        "<ul><li>Refresh an impersonation credential token.</li><li>Required roles: none</li></ul>"
    ],
    "patch:/imptoken": [
        "patch_imptoken",
        "Patch an impersonation credential token's refresh url and generate a new impersonation token.",
        "<ul>\n<li>Patch an impersonation credential token's refresh url and generate a new impersonation token.</li>\n<li>Required roles: none</li></ul>\n"
    ],
    "post:/impersonation-token": [
        "create_impersonation_token",
        "Create an impersonation token credential.",
        "<ul><li>Generate a credential token to impersonate user access for a set of subproject resources.</li><li>Required roles: app.trusted</li></ul>"
    ],
    "put:/impersonation-token": [
        "update_impersonation_token",
        "Refresh the impersonation credential token.",
        "<ul><li>Refresh an impersonation credential token.</li><li>Required roles: app.trusted</li></ul>"
    ],
    "post:/subproject/tenant/{tenantid}/subproject/{subprojectid}": [
        "create_new_subproject",
        "Create a new subproject.",
        "<ul>\n<li>Creates a new subproject resource in seismic store.</li>\n<li>Required roles: users.datalake.admin</li></ul>\n"
    ],
    "get:/subproject/tenant/{tenantid}/subproject/{subprojectid}": [
        "get_subproject_metadata",
        "Retrieve the subproject metadata.",
        "<ul><li>Return the metadata for a requested sub-project.</li><li>Required roles: subproject.admin</li></ul>"
    ],
    "delete:/subproject/tenant/{tenantid}/subproject/{subprojectid}": [
        "delete_subproject",
        "Delete a subproject.",
        "<ul><li>Delete a subproject in seismic store.</li><li>Required roles: users.datalake.admin</li></ul>"
    ],
    "patch:/subproject/tenant/{tenantid}/subproject/{subprojectid}": [
        "patch_subprojects_metadata",
        "Patch a subproject's metadata.",
        "<ul>\n  <li>Patch a subproject metadata in seismic store.</li>\n  <li>Required roles: subproject.admin</li>\n  <li>Possible actions:</li>\n  <ul>\n    <li>legal tag and/or ACLs groups can be patched by providing new values</li>\n  </ul>\n</ul>\n"
    ],
    "get:/subproject/tenant/{tenantid}": [
        "get_subproject_tenant",
        "List subprojects in a tenant.",
        "<ul><li>Return the list of sub-projects in a tenant.</li><li>Required roles: users.datalake.admin</li></ul>"
    ],
    "post:/tenant/{tenantid}": [
        "create_tenant",
        "Register a seismic-dms tenant.",
        "<ul><li>Register a data partition in seismic-dms.</li><li>Required roles: users.datalake.admin</li></ul>"
    ],
    "get:/tenant/{tenantid}": [
        "get_tenant",
        "Retrieve the tenant metadata.",
        "<ul><li>Return the tenant metadata.</li><li>Required roles: seistore.system.admin</li></ul>"
    ],
    "get:/tenant/sdpath": [
        "get_tenant_sdpath",
        "Retrieve the tenant seismic store path.",
        "<ul><li>Return the seistore path to a tenant associated with the data partition.</li><li>Required roles: none</li></ul>"
    ],
    "put:/user": [
        "update_user",
        "Add a user to a seismic store subproject authorization group.",
        "<ul><li>Add a user to a subproject default authorization group if it exists, otherwise, add the user to the first group.</li><li>Required roles: subproject.admin</li></ul>"
    ],
    "get:/user": [
        "get_user",
        "List users in a subproject's role-based authorization groups.",
        "<ul><li>List users in subproject's role-based authorization groups.</li><li>Required roles: subproject.admin</li></ul>"
    ],
    "delete:/user": [
        "delete_user",
        "Remove a user from a subproject.",
        "<ul><li>Remove a user from subproject default authorization groups if exists, otherwise, remove it from the first authorization group.</li><li>Required roles: subproject.admin</li></ul>"
    ],
    "get:/user/roles": [
        "get_user_roles",
        "Retrieve user role in all subprojects of the tenant.",
        "<ul><li>Retrieve user role in all subprojects of the tenant.</li><li>Required roles: none/li></ul>"
    ],
    "post:/app": [
        "create_app",
        "Register a new application.",
        "<ul><li>Register a new application in the seismic store.</li><li>Required roles: users.datalake.admin</li></ul>"
    ],
    "get:/app": [
        "get_app",
        "Retrieve the list of registered applications.",
        "<ul>\n<li>Retrieve the list of the registered applications in the seismic store.</li>\n<li>Required roles: users.datalake.admin</li></ul>\n"
    ],
    "post:/app/trusted": [
        "create_app_trusted",
        "Set a registered application as a trusted application.",
        "<ul>\n<li>Set a registered application as a trusted application in the seismic store.</li>\n<li>Required roles: users.datalake.admin</li></ul>\n"
    ],
    "get:/app/trusted": [
        "get_app_trusted",
        "List the trusted applications in a seismic store tenant.",
        "<ul>\n<li>Return the list of the trusted application in seismic store tenant.\n</li><li>Required roles: users.datalake.admin</li></ul>\n"
    ],
    "put:/operation/bulk-delete": [
        "update_operation_bulk_delete",
        "delete all datasets in a subproject path.",
        "<ul> <li>Description: delete all datasets in the specified sdms subproject path.</li> <li>Roles: subproject.admin</li></ul>"
    ],
    "get:/operation/bulk-delete/{operation-id}": [
        "get_operation_bulk_delete",
        "get the bulk delete operation status.",
        "<ul> <li>Description: get the bulk delete operation status.</li> <li>Roles: any (registered user in partition)</li></ul>"
    ],
    "put:/records": [
        "update_records",
        "Create or Update Records",
        "The API represents the main injection mechanism into the Data Ecosystem. \nIt allows records creation and/or update.When no record id is provided or when the provided id is not already present in the Data Ecosystem then a new record is created. \n If the id is related to an existing record in the Data Ecosystem then an update operation takes place and a new version of the record is created."
    ],
    "patch:/records": [
        "patch_records",
        "Modify record data and/or metadata attributes using patch operations",
        "The API represents the patch update mechanism for records. It allows updating multiple records in one request. The API supports metadata update only (Legal Tags, ACLs and Tags) if the request body media type is `application/json`. The API supports metadata and data update (Legal Tags, ACLs, Tags, Ancestry, Kind, Meta and Data) if the request body media type is `application/json-patch+json`. Please choose the appropriate media type from the Request body dropdown. The currently supported operations are replace, add, and remove. \nRequired roles: `users.datalake.editors` or `users.datalake.admins`."
    ],
    "put:/records/copy": [
        "copy_records_references",
        "Copy a Record references form one namespace to another",
        "This API attempts to copy all the Record references it is provided from the given source namespace to the target namespace. All refences will be copied or all will fail as a transaction. IF the target namesapce does not et exist it will be created. It requires 'services.storage.admin' permission to call"
    ],
    "post:/records/{id}:delete": [
        "delete_record",
        "Delete Record",
        "The API performs a logical deletion of the record using recordId. This operation can be reverted later. \nAllowed roles: `service.storage.creator` and `service.storage.admin` who is the OWNER of the record."
    ],
    "post:/records/delete": [
        "create_records_delete",
        "Soft delete of multiple records",
        "The API performs a soft deletion of the given list of records. \nRequired roles: `users.datalake.editors` or `users.datalake.admins` who is the OWNER of the record."
    ],
    "get:/query/records": [
        "query_records_from_kind",
        "Get all record from kind",
        "The API returns a list of all record ids which belong to the specified kind.\nAllowed roles: `service.storage.admin`."
    ],
    "post:/query/records": [
        "query_records",
        "Fetch records",
        "The API fetches multiple records at once.\nAllowed roles: `service.storage.viewer`,`service.storage.creator` and `service.storage.admin`."
    ],
    "post:/query/records:batch": [
        "get_records_batch",
        "Fetch multiple records",
        "The API fetches multiple records at once in the specific {data-partition-id}.The value of {frame-of-reference} indicates whether normalization is applied.\nRequired roles: `users.datalake.viewers` or `users.datalake.editors` or `users.datalake.admins`."
    ],
    "get:/records/{id}": [
        "get_record",
        "Get latest record version data",
        "This API returns the latest version of the given record.\nAllowed roles: `service.storage.viewer`, `service.storage.creator` and `service.storage.admin`."
    ],
    "delete:/records/{id}": [
        "purge_record",
        "Purge Record",
        "The API performs the physical deletion of the given record and all of its versions.\n This operation cannot be undone. \nAllowed roles: `service.storage.admin` who is the OWNER of the record."
    ],
    "get:/records/{id}/{version}": [
        "get_record_version",
        "Get Specific record",
        "The API retrieves the specific version of the given record. \nAllowed roles: `service.storage.viewer`, `service.storage.creator` and `service.storage.admin`."
    ],
    "get:/records/versions/{id}": [
        "get_record_versions",
        "Get record versions",
        "The API returns a list containing all versions for the given record id. \nAllowed roles: `service.storage.viewer`, `service.storage.creator` and `service.storage.admin`."
    ],
    "get:/liveness_check": [
        "get_liveness_check",
        "Liveness Check endpoint",
        "For deployment available public `/liveness_check` endpoint verifies the operational status of the Storage Service."
    ],
    "delete:/records/{id}/versions": [
        "purge_record_versions",
        "Purge Record Versions",
        "The API for the given record id, performs the permanent deletion of physical record versions excluding latest version. `versionIds`, `limit`, `from` query parameters used to delete the record versions. \n `versionIds` comma separated value of version ids can be provided on `versionIds` query parameter. API will delete all versions defined by 'versionIds' query parameter. Maximum 50 record versions can be deleted per request. If `limit` query parameter ONLY is used, then it will delete oldest versions defined by `limit`. If `from` query parameter is used then it will delete all versions before current one (exclusive). `versionIds` explicit version should always take precedence than `limit` & `from` query parameter If both `from` and `limit` are used then API will delete `limit` number of versions starting `from` version This operation cannot be undone. Required roles: `users.datalake.admins` who is the OWNER of the record."
    ],
    "get:/replay/status/{id}": [
        "get_replay_status",
        "get status of replay operation",
        "By passing replayId , you can get the replay operation status. \nAllowed roles: `users.datalake.ops`"
    ],
    "post:/replay": [
        "create_replay",
        "replay based on type",
        "Replay all the kinds. Allowed roles `users.datalake.ops`"
    ],
    "get:/whoami": [
        "get_whoami",
        null,
        ""
    ],
    "put:/whoami": [
        "update_whoami",
        null,
        ""
    ],
    "post:/whoami": [
        "create_whoami",
        null,
        ""
    ],
    "delete:/whoami": [
        "delete_whoami",
        null,
        ""
    ],
    "options:/whoami": [
        "options_whoami",
        null,
        ""
    ],
    "head:/whoami": [
        "head_whoami",
        null,
        ""
    ],
    "patch:/whoami": [
        "patch_whoami",
        null,
        ""
    ],
    "post:/query_with_cursor": [
        "query_with_cursor",
        "Queries the index using cursor for the input request criteria.",
        "The API supports full text search on string fields, range queries on date, numeric or string fields, along with geo-spatial search. \nRequired roles: `users.datalake.viewers` or `users.datalake.editors` or `users.datalake.admins` or `users.datalake.ops`. In addition, users must be a member of data groups to access the data. \nIt can be used to retrieve large numbers of results (or even all results) from a single search request, in much the same way as you would use a cursor on a traditional database."
    ],
    "post:/query": [
        "query",
        "Queries the index for the input request criteria.",
        "The API supports full text search on string fields, range queries on date, numeric or string fields, along with geo-spatial search. \n Required roles: `users.datalake.viewers` or  `users.datalake.editors` or `users.datalake.admins` or `users.datalake.ops`. In addition, users must be a member of data groups to access the data."
    ],
    "get:/readiness_check": [
        "get_readiness_check",
        "Readiness Check endpoint",
        "For deployment available public `/readiness_check` endpoint."
    ],
    "put:/partitions/provision": [
        "provision_partition",
        "Provision partition",
        "Provision partition. Required roles: `users.datalake.ops`"
    ],
    "post:/reindex": [
        "create_reindex",
        "Re-index given 'kind'",
        "This API allows users to re-index a 'kind' without re-ingesting the records via storage API. Required roles: `service.search.admin`"
    ],
    "patch:/reindex": [
        "patch_reindex",
        "Full Re-index by data partition",
        "This API allows users to re-index an entire partition without re-ingesting the records via storage API.Required roles: `users.datalake.ops`"
    ],
    "post:/reindex/records": [
        "reindex_given_records",
        "Re-index given records",
        "This API allows users to re-index the given records by providing record ids without re-ingesting the records via storage API. Required roles: `service.search.admin`"
    ],
    "delete:/index": [
        "delete_index",
        "Delete Index for the given kind",
        "Delete Index for the given kind. Required roles: `users.datalake.ops`"
    ],
    "put:/schemas/system": [
        "put_schemas_system",
        "Creates/Updates a schema in development status",
        "Creates a new schema or updates an already existing schema with status `DEVELOPMENT` in the schema repository. If a user tries to create a schema with status other than `DEVELOPMENT`, API will not throw an exception. <p>The update of schema without `DEVELOPMENT` status would cause error. Any schema instance with the same schemaIdentity is replaced. A schema state can also be changed from `DEVELOPMENT` to `PUBLISHED` or `OBSOLETE` while updating schema content or by providing the same schema content.</p> <p>**Note:** The schema may refer to other schema definitions in `DEVELOPMENT` state. If those schemas are updated themselves, it is the developer's responsibility to PUT the dependent schemas again to update the schema. Scope for a schema will be SHARED for all the schemas created using this API.</p><p>Service principal authorization is required to call thi API.</p>"
    ],
    "get:/schema": [
        "search_schema",
        "Searches SchemaInfo repository",
        "Searches for information of available schema (SchemaInfo) in schema repository. Support options to filter out the search contents. <p>Required roles:  `service.schema-service.viewers` groups to get the schema.</p>"
    ],
    "put:/schema": [
        "put_schema",
        "Creates/Updates a schema in development status",
        "Creates a new schema or updates an already existing schema with status `DEVELOPMENT` in the schema repository. If a user tries to create/update a schema with status other than `DEVELOPMENT`, API will throw an exception. <p>Any schema instance with the same schemaIdentity is replaced (in contrast to the immutability of `PUBLISHED` or `OBSOLETE` schemas). A schema state can also be changed from `DEVELOPMENT` to `PUBLISHED` or `OBSOLETE` while updating schema content or by providing the same schema content.</p> <p>**Note:** The schema may refer to other schema definitions in `DEVELOPMENT` state. If those schemas are updated themselves, it is the developer's responsibility to PUT the dependent schemas again to update the schemas. Scope for a schema can't be updated, its a system defined value.</p> <p>Required roles:  `service.schema-service.editors` groups to update schema.</p>"
    ],
    "post:/schema": [
        "create_schema",
        "Adds a schema to the schema repository.",
        "Adds a schema to the schema repository. The schemaIdentity must be unique. The `authority`, `source` and `entityType` will be registered if not present. <p>If lower minor versions are registered the service validates the new schema against breaking changes; if breaking changes are discovered the request fails.</p> <p>**Note:** The schema must not reference other schemas with status `DEVELOPMENT`. Scope to a schema will be set by system based on partition id (`SHARED` for common tenant and `INTERNAL` for private tenant). </p><p>Required roles : `service.schema-service.editors` groups to create schema.</p>"
    ],
    "get:/schema/{id}": [
        "get_schema",
        "Gets schema from the schema repository.",
        "Retrieve a schema using its system defined id. Required roles:  `service.schema-service.viewers` groups to get the schema."
    ],
    "get:/ddms/{id}": [
        "get_ddms",
        "Get a DDMS registration",
        "Get a DDMS registration with the given id. Required roles: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "delete:/ddms/{id}": [
        "delete_ddms",
        "Delete a DDMS registration",
        "Delete a DDMS registration with the given id. Required roles: 'users.datalake.admins'"
    ],
    "post:/ddms": [
        "create_ddms",
        "Create a DDMS registration",
        "Create a DDMS registration using an OpenApi spec V3 document. Required roles: 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "get:/ddms": [
        "query_ddms",
        "Query for DDMS registrations",
        "Query for DDMS registrations allowing retrievals by type. Required roles: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "get:/ddms/{id}/{type}/{localid}": [
        "get_d",
        "Retrieves Single Entity record id",
        "Get a Single DDMS record id. Required roles: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "get:/action/{id}": [
        "get_action",
        "Get an action registration",
        "Get an action registration with the given id. Required roles: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "delete:/action/{id}": [
        "delete_action",
        "Delete an action  registration",
        "Delete an action registration with the given id. Required role: 'users.datalake.admins'"
    ],
    "post:/action": [
        "create_action",
        "Create an action registration",
        "Create an action registration. Required role: 'users.datalake.admins'"
    ],
    "post:/action:test": [
        "create_action_test",
        "Test an action registration",
        "Test an action registration. Required roles: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "post:/action:retrieve": [
        "create_action_get",
        "Query for action registrations and substitutes any action with the given parameters",
        "Retrieve an action registration. Required roles: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "post:/subscription": [
        "create_subscription",
        "Create a subscription",
        "Create a subscription. Required roles: 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "get:/subscription/{id}": [
        "get_subscription",
        "Get a subscription",
        "Get a subscription with its Id. Required role: 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "delete:/subscription/{id}": [
        "delete_subscription",
        "Delete a subscription",
        "Delete a subscription with its Id. Required role: 'users.datalake.admins'"
    ],
    "put:/subscription/{id}/secret": [
        "update_subscription_secret",
        "Update secret for subscription",
        "Update secret for a subscription. Required role: 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "get:/topics": [
        "list_topics",
        "List all topics",
        "List all topics that you can create a subscription for, along with the corresponding sample messages. Required role: 'users.datalake.editors' or 'users.datalake.admins'"
    ],
    "get:/partitions/{partitionId}": [
        "get_partition",
        "Get Partition Info",
        "Get all properties and their values for a given data partition id"
    ],
    "post:/partitions/{partitionId}": [
        "create_partition",
        "Create a new Partition",
        "Creates a new data partition with all given properties and their values."
    ],
    "delete:/partitions/{partitionId}": [
        "delete_partition",
        "Delete an existing Partition",
        "Delete all the properties of a given data partition"
    ],
    "patch:/partitions/{partitionId}": [
        "patch_partition",
        "Updates an existing Partition",
        "Add new properties or update existing properties of a given data partition"
    ],
    "get:/partitions": [
        "list_partitions",
        "List all Partitions",
        "Returns all existing data partitions"
    ],
    "get:/projects": [
        "list_projects",
        "Get all project's.",
        "This API returns a list of projects."
    ],
    "post:/projects": [
        "create_projects",
        "Create new collaboration project.",
        "The API performs new collaboration project creation."
    ],
    "post:/projects/{id}/status": [
        "create_projects_status",
        "Change status of collaboration project by project id.",
        "The API to change status by project Id."
    ],
    "get:/projects/{id}/resources": [
        "get_projects_resources",
        "Get collaboration project resources Ids by project Id.",
        "The API returns the given record by project Id."
    ],
    "post:/projects/{id}/resources": [
        "create_projects_resources",
        "Assign the resources to the collaboration project.",
        "The API performs assignment of resources to the collaboration project."
    ],
    "delete:/projects/{id}/resources": [
        "delete_projects_resources",
        "Revoke the resources from the collaboration project by project id.",
        "The API revoke resources by project Id."
    ],
    "get:/projects/{id}/lifecycleevent": [
        "get_projects_lifecycleevent",
        "Read LifecycleEvents from the collaboration project.",
        "The API performs read operation of LifecycleEvents from the collaboration project."
    ],
    "post:/projects/{id}/lifecycleevent": [
        "create_projects_lifecycleevent",
        "Assign the LifecycleEvent to the collaboration project.",
        "The API performs assignment of LifecycleEvents to the collaboration project."
    ],
    "delete:/projects/{id}/lifecycleevent": [
        "delete_projects_lifecycleevent",
        "Delete LifecycleEvents from the collaboration project.",
        "The API performs delete operation of LifecycleEvents from the collaboration project."
    ],
    "get:/projects/{id}": [
        "get_project",
        "Get project by Id.",
        "The API returns the given record by its Id."
    ],
    "get:/projects/{id}/wip-resources": [
        "get_projects_wip_resources",
        "Get collaboration project WIP resources Ids by project Id.",
        "The API returns the given record by project Id."
    ],
    "post:/push-handlers/records-changed": [
        "record_changed",
        "recordChanged",
        ""
    ],
    "post:/v2/getLocation": [
        "create_v2_get_location",
        "Get a location in Landing Zone to upload a file.",
        "Create a new location in the landing zone to upload a file.\n\n**Required roles**: 'users.datalake.editors' or 'users.datalake.admins' or 'users.datalake.ops'.\n"
    ],
    "get:/v2/files/uploadURL": [
        "get_v2_files_upload_u_r_l",
        "Get a location in Landing Zone to upload a file.",
        "Gets a temporary signed URL to upload a file.The generated URL is time bound and expires after 24 hours.\n\nUser will receive a FileSource in the response.This is the relative path where the uploaded file will persist.\nOnce the file is uploaded, FileSource can then be used to post metadata of the file. The uploaded file gets automatically deleted, if the metadata is not posted within 24 hours of uploading the file.\n\n\n**Required roles**: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins' or 'users.datalake.ops'."
    ],
    "post:/v2/files/metadata": [
        "create_v2_files_metadata",
        "Creates a metadata for a file",
        "This API creates a metadata record for a file that is already uploaded. The Metadata is linked to the file via `FileSource` provided in the request body.\n\nIf `FileSource` attribute is missing in the request body or there is no file present, then the request fails with an error.\n\nWhen metadata is successfully updated in the system, it returns the `Id` of the file metadata record.\n\n**Required roles**: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins' or 'users.datalake.ops'."
    ],
    "get:/v2/files/{Id}/metadata": [
        "get_v2_files_metadata",
        "Gets metadata record for the given id",
        "Gets the latest version of File metadata record identified by the given id.\n\n**Required roles**: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins' or 'users.datalake.ops'."
    ],
    "delete:/v2/files/{Id}/metadata": [
        "delete_v2_files_metadata",
        "Deletes metadata record & file assocaited with that record for the given id",
        "Deletes the File metadata record identified by the given id and file associated with that metadata record.\n\n  **Required roles**: 'users.datalake.editors'  or 'users.datalake.admins'."
    ],
    "get:/v2/files/{Id}/downloadURL": [
        "gets_url_to_download_file",
        "Gets a URL to download the file",
        "Gets a URL for downloading the file associated with the unique `id`.\n\n**Required roles**: 'users.datalake.viewers' or 'users.datalake.editors' or 'users.datalake.admins' or 'users.datalake.ops'."
    ],
    "post:/v2/getFileLocation": [
        "create_v2_get_file_location",
        null,
        "Returns file `Location` and `Driver`.\n"
    ],
    "post:/v2/delivery/getFileSignedUrl": [
        "create_v2_delivery_get_file_signed_url",
        null,
        "Returns delivery instructions for File(s) using SRNs\n"
    ],
    "post:/v2/getFileList": [
        "create_v2_get_file_list",
        null,
        "Allows the application to audit the attempted file uploads. The method is internal and isn't available for third-party applications.\n"
    ],
    "get:/v2/info": [
        "get_v2_info",
        "Version info",
        "For deployment available public `/info` endpoint,  which provides build and git related information."
    ],
    "post:/v2/file-collections/storageInstructions": [
        "create_v2_file_collections_storage_instructions",
        null,
        "get storage/upload location file-collection datasets"
    ],
    "post:/v2/file-collections/retrievalInstructions": [
        "create_v2_file_collections_retrieval_instructions",
        null,
        "Generate retrieval instructions (Eg - Signed URLs) for datasets"
    ],
    "post:/v2/file-collections/copy": [
        "create_v2_file_collections_copy",
        null,
        "Copy file collection from "
    ],
    "put:/registerDataset": [
        "update_register_dataset",
        "Create or Update Dataset Registry",
        "Create or Update Dataset Registry. \n**Required roles: `service.storage.creator` or `service.storage.admin`."
    ],
    "post:/storageInstructions": [
        "create_storage_instructions",
        "Generate storage instructions ",
        "Generate storage instructions (Eg - Signed URLs) for datasets. \nRequired roles: `service.dataset.editors`."
    ],
    "post:/revokeURL": [
        "create_revoke_u_r_l",
        "${datasetDmsAdminApi.revokeURL.summary}",
        "${datasetDmsAdminApi.revokeURL.description}"
    ],
    "get:/retrievalInstructions": [
        "get_retrieval_instructions",
        "Generate retrieval instructions",
        "Generate retrieval instructions (Eg - Signed URLs) for single dataset. \nRequired roles: `service.dataset.viewers`."
    ],
    "post:/retrievalInstructions": [
        "create_retrieval_instructions",
        "Generate retrieval instructions - multiple datasets ",
        "Generate retrieval instructions (Eg - Signed URLs) for multiple datasets. \nRequired roles: `service.dataset.viewers`."
    ],
    "get:/getDatasetRegistry": [
        "get_dataset_registry",
        "Get Dataset Registry",
        "Get Dataset Registry. \n**Required roles:  `service.storage.creator` or `service.storage.admin` or `service.storage.viewer`."
    ],
    "post:/getDatasetRegistry": [
        "get_dataset_registries",
        "Get Dataset Registries",
        "Get Dataset Registries. \n**Required roles:  `service.storage.creator` or `service.storage.admin` or `service.storage.viewer`."
    ],
    "get:/legaltags": [
        "list_legaltags",
        "Gets all LegalTags.",
        "This allows for the retrieval of all LegalTags."
    ],
    "put:/legaltags": [
        "update_legaltags",
        "Updates the LegalTag for the given `name`.",
        "This allows to update certain properties of your LegalTag using the `name` associated with it."
    ],
    "post:/legaltags": [
        "create_legaltags",
        "Creates the LegalTag for the given `name`.",
        "This allows for the creation of your LegalTag. There can only be 1 LegalTag per `name`. A LegalTag must be created before you can start ingesting data for that name."
    ],
    "post:/legaltags:validate": [
        "create_legaltags_validate",
        "Retrieves the invalid LegalTag names with reasons for the given `names`.",
        "This allows for the retrieval of the reason why your LegalTag is not valid. A maximum of 25 can be retrieved at once."
    ],
    "post:/legaltags:query": [
        "create_legaltags_query",
        "Retrieves the legaltags which matches search criteria or none if there is no match",
        "This allows search for specific attributes of legaltags including the attributes of extensionproperties"
    ],
    "post:/legaltags:batchRetrieve": [
        "create_legaltags_batch_get",
        "Retrieves the LegalTags for the given `names`.",
        "This allows for the retrieval of your LegalTags using the `name` associated with it. A maximum of 25 can be retrieved at once."
    ],
    "get:/legaltags:properties": [
        "get_legaltags_properties",
        "Gets LegalTag property values.",
        "This allows for the retrieval of allowed values for LegalTag properties."
    ],
    "get:/legaltags/{name}": [
        "get_legaltag",
        "Gets a LegalTag for the given `name`.",
        "This allows for the retrieval of your LegalTag using the `name` associated with it."
    ],
    "delete:/legaltags/{name}": [
        "delete_legaltags",
        "Deletes a LegalTag for the given `name`.",
        "This allows for the deletion of your LegalTag with the given `name`. This makes the given legaltags data invalid."
    ],
    "get:/jobs/updateLegalTagStatus": [
        "get_jobs_update_legal_tag_status",
        "Check LegalTag Compliance Job Status",
        "To check LegalTag Compliance Job Status."
    ],
    "get:/": [
        "home_page",
        "Home Page",
        ""
    ],
    "get:/api/policy/v1/policies": [
        "get_api_policy_v1_policies",
        "Fetch All Policies That Match Partition",
        "Return all policies from OPA directly that match partition_bundle_root data-partition-id in header (if bundles are enabled).\nThis API gives the list of all the defined policies and it includes the policy definitions in the raw Rego form.\nIt performs authorization check. The user making the call needs to be either service.policy.user or service.policy.admin in the provided data partition."
    ],
    "get:/api/policy/v1/policies/{policy_id}": [
        "get_fetch_policy",
        "Fetch A Policy",
        "Return a policy directly from OPA with no filtering"
    ],
    "get:/api/policy/v1/policies/osdu/instance/{policy_id}": [
        "get_fetch_instance_policy",
        "Fetch Instance Policy",
        "Return an instance policy from OPA directly."
    ],
    "get:/api/policy/v1/policies/osdu/partition/{data_partition}/{policy_id}": [
        "get_api_policy_v1_policies_osdu_partition",
        "Fetch Partition Policy Directly From Opa",
        "Return an policy for a partition id from OPA.\nRequires data-partition-id in header."
    ],
    "delete:/api/policy/v1/policies/osdu/partition/{data_partition}/{policy_id}": [
        "delete_partition_policy",
        "Delete Partition Policy",
        "## Delete a partition policy\n* This API requires admin privileges (service.policy.admin) in the provided data partition.\n* Partition ID in header and in path must match\n* Policy_id must end with a \".rego\"\n\n### Possible http return status codes:\n* 200 - no error\n* 202 - accepted\n* 400 - bad request\n    * for example, if data_partition in path doesn't match data-partition-id in header\n* 401 - unauthorized\n* 403 - forbidden\n    * for example, if calling with only user privs\n* 404 - not found\n* 422 - validation Error\n    * for example, if policy_id doesn't end with \".rego\"\n* 500 - server error\n* 501 - not implemented\n    * for example, if bundles are not supported.\n* 503 - service not available\n    * for example, if issues with bundles server\n\nErrors will include some detail in returning json.\n\nReturn json:\n```\n    {\n        \"policy_id\": string, \n        \"data_partition\": string,\n        \"status\": bool,\n        \"message\": string,\n        \"result\": string json from OPA\n    }\n```"
    ],
    "put:/api/policy/v1/policies/osdu/partition/{data_partition}/{policy_id}": [
        "create_or_update_partition_policy",
        "Create Or Update Partition Policy",
        "## Create or update a policy with given policy_id for a given data partition.\nThis API will create/update policy definition with provided Rego expression included in file and assign it with provided id.\n\n* This API requires admin privileges (service.policy.admin) in the provided data partition.\n* Partition ID in header and in path must match\n* Policy_id must end with a \".rego\"\n\n### Possible http return status codes:\n* 200 - no error\n* 202 - accepted\n* 400 - bad request\n    * for example, if data_partition in path doesn't match data-partition-id in header\n* 401 - unauthorized\n* 403 - forbidden\n    * for example, if calling with only user privs\n* 422 - validation Error\n    * for example, if policy_id doesn't end with \".rego\"\n    * for example, if package declaration issue\n* 500 - server error\n* 501 - not implemented\n    * for example, if bundles are not supported.\n* 503 - service not available\n    * for example, if issues with bundles server\n\nErrors will include some detail in returning json.\n\nReturn json:\n```\n    {\n        \"policy_id\": string,\n        \"data_partition\": string,\n        \"opa_payload\": string,\n        \"status_code\": http status code\n        \"status\": bool\n        \"message\": string\n    }\n```"
    ],
    "post:/api/policy/v1/evaluations/query": [
        "evaluate_policy",
        "Evaluate Policy",
        "## Evaulate Policies\nThis API is to help you evaluate policies.\n\nIf include_auth is True, then in your file data token, xuserid and data partition id will be ignored in the file and information from the headers\nof the request will be used for this information.\n\n### Example:\nFor example file data for policy dataauthz.rego:\nWhere XXXX is the data partition and YYYY is a legal tag\n```json\n{\n    \"input\": {\n        \"operation\": \"update\",\n        \"records\": [\n            {\n                \"id\":\"XXXX:test:1.4.1654807204111\",\n                \"kind\":\"XXXX:bulkupdate:test:1.1.1654807204111\",\n                \"legal\":{\n                    \"legaltags\":[\n                        \"YYYY\"\n                    ],\n                    \"otherRelevantDataCountries\":[\"US\"],\n                    \"status\":\"compliant\"\n                },\n                \"acls\":{\n                    \"viewers\":[\"data.default.viewers@XXXX.group\"],\n                    \"owners\":[\"data.default.owners@XXXX.group\"]\n                }\n            }\n        ]\n    }\n}\n```"
    ],
    "post:/api/policy/v1/translate": [
        "translate_policy_api",
        "Translate Policy Api",
        "## Translate policy\nGiven an OPA query that should be partially evaluated, return an ElasticSearch request body\n\nIn the body of the request the JSON schema should match \"TranslateItem\".\nPlease note: xuserid, token and datapartitionid are now actively inserted into input request"
    ],
    "get:/api/policy/v1/info": [
        "get_api_policy_v1_info",
        "Return Version Info",
        "Return Service version information.\nExpected returned JSON is in \"InfoOut\" schema, which include Services and ServiceDetail schemas."
    ],
    "post:/api/policy/v1/compile": [
        "create_api_policy_v1_compile",
        "Compile Partially Evaluate A Query",
        "# Compile - Partially evaluate a query.\nThe Compile API allows you to partially evaluate Rego queries and obtain a simplified version of the policy.\n\n### Metrics\nWhen query parameter metrics=true, the API response will include detailed performance metrics from OPA.\nOPA currently supports the following query performance metrics:\n\n    timer_rego_input_parse_ns: time taken (in nanoseconds) to parse the input\n    timer_rego_query_parse_ns: time taken (in nanonseconds) to parse the query.\n    timer_rego_query_compile_ns: time taken (in nanonseconds) to compile the query.\n    timer_rego_query_eval_ns: time taken (in nanonseconds) to evaluate the query.\n    timer_rego_module_parse_ns: time taken (in nanoseconds) to parse the input policy module.\n    timer_rego_module_compile_ns: time taken (in nanoseconds) to compile the loaded policy modules.\n    timer_server_handler_ns: time take (in nanoseconds) to handle the API request.\n\n### Instrumentation\nTo enable query instrumentation, specify metrics=true and instrument=true query parameters when executing the API call.\nQuery instrumentation can help diagnose performance problems, however, it can add significant overhead to query evaluation.\nWe recommend leaving query instrumentation off unless you are debugging a performance problem."
    ],
    "get:/api/policy/v1/tenant": [
        "get_tenant",
        "Get Tenant",
        "Experimental tenant API for retrieving OPA bundle config for a data partition.\nThese details are read from OPA configmap."
    ],
    "put:/api/policy/v1/tenant": [
        "update_tenant",
        "Update Tenant",
        "Experimental tenant API for updating OPA bundle config for a data partition.\nAdding new partitions is not supported in M20."
    ],
    "delete:/api/policy/v1/tenant": [
        "delete_tenant",
        "Delete Tenant",
        "Experimental tenant API for deleting tenant OPA bundle config for a data partition.\nDeleting partitions is not supported in M20."
    ],
    "get:/api/policy/v1/health": [
        "get_health",
        "Health",
        "## Health check endpoint, which does not depend on OPA.\nThis API does not require any headers or authentication.\n\nThe /health endpoint responds with a 200 HTTP status code when the service pod can receive requests.\nThe endpoint indicates that the service pod is healthy and reachable.\nIt does not indicate that the service is ready to serve requests."
    ],
    "get:/api/policy/v1/ready": [
        "get_ready",
        "Ready",
        "## Health check endpoint, which depends on OPA being available and healthy.\nThis API does not require any headers or authentication.\n\n### Possible http return status codes:\n* 200 - no error\n* 501 - not implemented\n* 503 - service not available\n\nThe /ready endpoint responds with a 200 HTTP status code if the overall application works.\nThe endpoint indicates that the service is ready to serve requests."
    ],
    "put:/api/policy/v1/validate/{policy_id}": [
        "validate_policy",
        "Validate Policy",
        "# Validate Policy\nThis API checks to make sure the rego is valid and the naming of the policy package is acceptable.\n\nIf template parameter is True, then the incoming file will automatically replace the following during validation:\n- data_partition\n- DATA_PARTITION\n- name with policy_id without \".rego\" suffix"
    ],
    "get:/api/policy/v1/backup": [
        "get_backup",
        "Backup",
        "Experimental Backup API.\n\nAllows downloading the bundle for a data partition.\n\nBundle filename will be in the form bundle-`data partition`-`date`.tar.gz"
    ],
    "post:/api/policy/v1/bootstrap": [
        "bootstrap",
        "Bootstrap",
        "Experimental bootstrap API for creating and updating bundle to default.\nThis should be used when adding a partition to OSDU.\n\nWithout force:\n\n    * This method is only allowed if the partition doesn't already have a bundle.\n    * If the bundle already exists it will return 405 METHOD_NOT_ALLOWED.\n    * Policy Service can be configured to ignore force.\n\nMay return:\n\n    * HTTP_202_ACCEPTED - updated\n    * HTTP_201_CREATED - created\n    * HTTP_405_METHOD_NOT_ALLOWED - not allowed\n    * HTTP_424_FAILED_DEPENDENCY - bundle server caused failure"
    ],
    "get:/api/policy/v1/config": [
        "get_api_policy_v1_config",
        "Show Policy Config Details",
        "Return detail configuration details.\nDiagnostic API"
    ],
    "get:/secrets/{secret_name}": [
        "get_secret",
        null,
        ""
    ],
    "put:/secrets/{secret_name}": [
        "update_secret",
        null,
        ""
    ],
    "delete:/secrets/{secret_name}": [
        "delete_secret",
        null,
        ""
    ],
    "get:/secrets": [
        "list_secrets",
        null,
        ""
    ],
    "post:/secrets": [
        "create_secrets",
        null,
        ""
    ],
    "post:/secrets:retrieve": [
        "retrieve_secrets",
        null,
        ""
    ],
    "post:/secrets/recover/{secret_name}": [
        "recover_secret",
        null,
        ""
    ],
    "get:/secrets/deleted/{secret_name}": [
        "get_deleted_secrets",
        null,
        ""
    ],
    "get:/health": [
        "get_health",
        null,
        ""
    ]
}