# generated by datamodel-codegen:
#   filename:  swagger.yaml
#   timestamp: 2024-07-09T15:55:12+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field

from .FileCollection.SEGY.field_1 import field_0 as field_0_1
from .FileCollection.Slb.OpenZGY.field_1 import field_0


class Status(BaseModel):
    status: str


class Info(BaseModel):
    group_id: Optional[str] = None
    artifact_id: Optional[str] = None
    build_time: Optional[str] = None
    connected_outer_services: Optional[List] = None


class DatasetListBody(BaseModel):
    type: Optional[str] = None
    gtags: Optional[List[str]] = Field(
        None,
        description='Array of global tags associated with the dataset metadata. Once assigned, they can be used to filter datasets.',
    )
    search: Optional[str] = None
    filter: Optional[Dict[str, Any]] = None
    limit: Optional[float] = Field(
        None, description='the maximum number of datasets in the response'
    )
    cursor: Optional[str] = Field(
        None,
        description='the cursor value required to retrieve the next page of result',
    )


class DatasetLsBody(BaseModel):
    sdpath: str = Field(
        ...,
        description='the seismic dms path uri: sd://tenant or sd://tenant/subproject or sd://tenant/subproject/path.',
    )
    wmode: Optional[str] = Field(
        None,
        description='the endpoint working mode: dirs, datasets, all (default if not specified).',
    )
    limit: Optional[float] = Field(
        None, description='the max response datasets number.'
    )
    cursor: Optional[str] = Field(None, description='the next page cursor.')


class Seismicmeta(BaseModel):
    kind: Optional[str] = None
    data: Optional[Dict[str, Any]] = None


class Acls(BaseModel):
    admins: Optional[List[str]] = None
    viewers: Optional[List[str]] = None


class Dataset(BaseModel):
    name: str = Field(..., description='Name of the dataset.')
    tenant: str = Field(..., description='Name of the tenant.')
    subproject: str = Field(..., description='Name of the subproject.')
    path: str = Field(..., description='Path to the dataset inside a subproject.')
    created_by: str = Field(..., description='Creator of the dataset.')
    created_date: str = Field(..., description='Date when the dataset was created.')
    last_modified_date: str = Field(
        ..., description='Date when the dataset was last modified.'
    )
    filemetadata: Optional[Dict[str, Any]] = Field(
        None, description='Number of objects and the size in bytes of the dataset.'
    )
    metadata: Optional[Dict[str, Any]] = Field(
        None,
        description='Generic information stored as key,value pairs for the dataset.',
    )
    gcsurl: str = Field(..., description='Cloud storage object identifier.')
    readonly: Optional[bool] = Field(
        None, description='True if the dataset is readonly.'
    )
    status: Optional[str] = Field(None, description='The dataset status (if set).')
    ltag: Optional[str] = Field(
        None, description='Legal tag associated with the dataset.'
    )
    ctag: str = Field(
        ...,
        description='The coherency tag associated with the dataset. (It changes every time the dataset is updated.)',
    )
    seismicmeta_guid: Optional[str] = Field(
        None, description='Seismic storage record identifier.'
    )
    sbit: Optional[str] = Field(None, description='The session lockID.')
    sbit_count: Optional[float] = Field(
        None, description='The number of sessions associated to the dataset.'
    )
    seismicmeta: Optional[Seismicmeta] = Field(
        None,
        description='Seismic metadata associated with the dataset which is used to create a data ecosystem storage record.',
    )
    openzgy_v1: Optional[field_0.Field0] = None
    segy_v1: Optional[field_0_1.Field0] = None


class DatasetPatch(BaseModel):
    dataset_new_name: Optional[str] = Field(
        None, description='New name for the dataset.'
    )
    metadata: Optional[Dict[str, Any]] = Field(
        None,
        description='Generic information about the dataset stored as key value pairs.',
    )
    filemetadata: Optional[Dict[str, Any]] = Field(
        None, description='Number of objects and the size in bytes of the dataset.'
    )
    last_modified_date: Optional[str] = Field(
        None, description='Date when the dataset was last modified.'
    )
    gtags: Optional[List[str]] = Field(
        [],
        description='Array of tags associated with the dataset. After patching these tags, they can be used to filter the datasets.',
    )
    ltag: Optional[str] = Field(
        None, description='Legal tag associated with the dataset.'
    )
    readonly: Optional[bool] = Field(
        None, description='True if the dataset is read only.'
    )
    status: Optional[str] = Field(
        None, description='The status of the dataset (if set)'
    )
    seismicmeta: Optional[Seismicmeta] = Field(
        None,
        description='Seismic metadata associated with the dataset which is used to create a data ecosystem storage record.',
    )
    openzgy_v1: Optional[field_0.Field0] = None
    segy_v1: Optional[field_0_1.Field0] = None
    acls: Optional[Acls] = Field(
        None, description='ACLs with admin groups and viewer groups for the dataset.'
    )


class DatasetPermission(BaseModel):
    read: bool = Field(
        ..., description='True if the user has read permission on the dataset.'
    )
    write: bool = Field(
        ..., description='True if the user has write permission on the dataset.'
    )
    delete: bool = Field(
        ..., description='True if the user has delete permission on the dataset.'
    )


class DatasetAndDirectories(BaseModel):
    datasets: List[str] = Field(
        ..., description='Array of datasets inside the subproject.'
    )
    directories: List[str] = Field(
        ..., description='Array of directories inside the subproject.'
    )


class DatasetCheckList(BaseModel):
    datasets: List[str] = Field(
        ..., description='Array of datasets inside the subproject.'
    )


class DatasetBulkDeleteBody(BaseModel):
    filter: Optional[Dict[str, Any]] = Field(
        None,
        description='Optional structured query filter to restrict the datasets to be deleted.',
    )


class AccessToken(BaseModel):
    access_token: str = Field(
        ...,
        description='connection string credentials or standard access token (CSP dependent)',
    )
    token_type: str = Field(..., description='token type (Bearer, SasUrl, ....).')
    expires_in: float = Field(
        ...,
        description='expiration time (in minutes) of the connection string credentials',
    )


class ImpersonationToken(BaseModel):
    impersonation_token: str = Field(..., description='The Impersonation token.')
    token_type: str = Field(..., description='Token type.')
    expires_in: float = Field(..., description='Token expiration time.')
    context: str = Field(..., description='the Impersonation token context.')


class Resource(BaseModel):
    resource: str
    readonly: Optional[bool] = None


class ImpersonationTokenRequest(BaseModel):
    resources: List[Resource]
    metadata: Optional[Dict[str, Any]] = Field(
        None,
        description='A general field to set extra meta-information to the impersonation token signature. These are saved along with the token signatures.',
    )


class ImpTokenRequest(BaseModel):
    token: str = Field(
        ..., description='Impersonation token that was previously issued.'
    )
    resources: List[Resource]
    refresh_url: str = Field(
        ...,
        alias='refresh-url',
        description='A web service endpoint that the seismic store service will invoke to check if the impersonation token can be refreshed.',
    )


class ImpTokenPatchRequest(BaseModel):
    token: str = Field(
        ..., description='Impersonation token that was previously issued.'
    )
    refresh_url: str = Field(
        ...,
        alias='refresh-url',
        description='New endpoint that the seismic store service will invoke to check if the impersonation token can be refreshed.',
    )


class RefreshTokenRequest(BaseModel):
    token: str = Field(..., description='Impersonation token')


class AccessPolicy(Enum):
    uniform = 'uniform'
    dataset = 'dataset'


class SubProjectCreateBody(BaseModel):
    admin: Optional[str] = Field(
        None,
        description='An optional user to set as admin. The user will be added in the first admins acl group',
    )
    storage_class: Optional[str] = Field(
        None, description='Storage class for the bucket (Google Required Only)'
    )
    storage_location: Optional[str] = Field(
        None, description='Storage location for the bucket (Google Required Only)'
    )
    access_policy: Optional[AccessPolicy] = Field(
        'uniform',
        description='The datasets access level mode: "uniform" (uniform data access to all subprojects datasets) or "datasets" (acl can be applied at dataset level)',
    )
    acls: Optional[Acls] = Field(
        None,
        description='The entitlement groups to enable subproject access as admin or viewer. If not specified, default entitlement data groups will be created.',
    )


class SubProjectPatchBody(BaseModel):
    access_policy: Optional[str] = Field(
        'dataset', description='Access Policy for the subproject.'
    )
    acls: Optional[Acls] = Field(
        None,
        description='ACLs with admin groups and viewer groups. Existing acls will be replaced with the provided acls.',
    )


class SubProject(BaseModel):
    name: str = Field(..., description='Name of the subproject.')
    tenant: str = Field(..., description='Name of the tenant.')
    storage_class: str = Field(..., description='Storage class for the bucket.')
    storage_location: str = Field(..., description='Storage location for the bucket.')
    ltag: str = Field(..., description='Legal tag for the subproject.')
    access_policy: AccessPolicy = Field(
        ..., description='Access policy for the subproject.'
    )
    acls: Optional[Acls] = Field(
        None, description='ACLs with admin groups and viewer groups for the subproject.'
    )


class TenantCreateBody(BaseModel):
    gcpid: str = Field(
        ...,
        description='Google cloud project id associated with the tenant. For other providers, name of the data partition.',
    )
    esd: str = Field(
        ...,
        description='Entitlements group Sub Domain (ESD). For instance, if the entitlements group is users.datalake.viewers@{datapartition}.{domain}.com, the esd value is {datapartition}.{domain}.com. It must start with the name of the data partition.',
    )
    default_acls: str = Field(
        ...,
        description='Entitlements authorization group to manage tenant administrators.',
    )


class Tenant(BaseModel):
    name: str = Field(..., description='Name of the tenant.')
    esd: str = Field(
        ...,
        description='Entitlements group Sub Domain (ESD). For instance, if the entitlements group is users.datalake.viewers@{datapartition}.{domain}.com, the esd value is {datapartition}.{domain}.com. It has to start with the name of the data partition.',
    )
    gcpid: str = Field(
        ...,
        description='Google cloud project id associated with the tenant. For other providers, name of the data partition.',
    )
    default_acls: str = Field(
        ...,
        description='Entitlements authorization group to manage tenant administrators.',
    )


class Group(Enum):
    viewer = 'viewer'
    admin = 'admin'


class UserAddRequest(BaseModel):
    email: str = Field(
        ...,
        description='Email address of the user. Note that this field is used as a principal identifier for entitlements-svc and can be configured using the env variable USER_ID_CLAIM_FOR_ENTITLEMENTS_SVC in the Seismic DMS runtime.',
    )
    path: str = Field(
        ..., description='Seismic store path in the format sd://tenant/subproject.'
    )
    group: Group = Field(..., description='Role to be assigned to the user.')


class UserRemoveRequest(BaseModel):
    email: str = Field(
        ...,
        description='Email address of the user. Note that this field is used as a principal identifier for entitlements-svc and can be configured using the env variable USER_ID_CLAIM_FOR_ENTITLEMENTS_SVC in the Seismic DMS runtime.',
    )
    path: str = Field(
        ..., description='Seismic store path in the format sd://tenant/subproject.'
    )


class UserRoles(BaseModel):
    roles: List[List[str]] = Field(
        ...,
        description='Array of arrays with each array containing subproject and role of the user for the subproject.',
        example=[['/spx01', 'admin'], ['/spx02', 'viewer']],
    )


class LsDatasets(BaseModel):
    __root__: List[str] = Field(
        ...,
        description='Array of directories and datasets inside a subproject.',
        example=['folderA/', 'folderB/', 'dataset01', 'dataset02', 'dataset03'],
    )


class LsPaginatedDatasets(BaseModel):
    datasets: List[str] = Field(
        ..., description='Array of datasets inside the subproject.'
    )
    nextPageCursor: str = Field(..., description='Next cursor for pagination.')


class PaginatedDatasets(BaseModel):
    datasets: Optional[List[Dataset]] = None
    nextPageCursor: Optional[str] = Field(
        None, example='next-page-token-3rt431v41-4515351'
    )


class DatasetSize(BaseModel):
    computed_size: float = Field(..., description='The size of the dataset in bytes..')
    computed_size_date: str = Field(
        ..., description='The date of when the dateset size was computed.'
    )


class ComputedSize(BaseModel):
    size_bytes: float = Field(..., description='The size of the datasets in bytes.')
    dataset_count: float = Field(
        ...,
        description='The number of datasets in the path (including nested folders).',
    )


class Operation(BaseModel):
    operation_id: str = Field(..., description='Operation id of the operation job.')


class OperationStatus(BaseModel):
    operation_id: str = Field(..., description='Operation id of the operation job.')
    created_at: Optional[str] = Field(
        None, description='Time when the operation was initiated.'
    )
    created_by: Optional[str] = Field(
        None, description='User who initiated the operation job.'
    )
    last_updated_at: Optional[str] = Field(
        None, description='Time when the operation status was last updated.'
    )
    status: str = Field(..., description='Status of the operation job.')
    dataset_cnt: Optional[float] = Field(
        None, description='Total number of datasets to process.'
    )
    completed_cnt: Optional[float] = Field(
        None, description='Number of datasets correctly processed.'
    )
    failed_cnt: Optional[float] = Field(
        None, description='Number of datasets which processing failed.'
    )


class DatasetRegisterBody(BaseModel):
    type: Optional[str] = None
    gtags: Optional[List[str]] = Field(
        None,
        description='Array of global tags associated with the dataset metadata. Once assigned, they can be used to filter datasets.',
    )
    seismicmeta: Optional[Seismicmeta] = Field(
        None,
        description='Seismic metadata to be stored as dataecosystem storage record.',
    )
    openzgy_v1: Optional[field_0.Field0] = None
    segy_v1: Optional[field_0_1.Field0] = None
    acls: Optional[Acls] = Field(
        None, description='ACLs with admin groups and viewer groups for the dataset.'
    )
